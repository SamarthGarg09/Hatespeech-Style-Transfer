{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building word-peice tokenizer from scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenizer object with a model, then set its `normalizer`, `pre_tokenizer`, `post_processor`, and `decoder` attributes to the values we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers  import (\n",
    "    decoders, \n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers, \n",
    "    processors, \n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "# normalization\n",
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()\n",
    "])\n",
    "# pre-tokenization\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([\n",
    "    pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()\n",
    "])\n",
    "# trainer\n",
    "special_tokens = ['[UNK]', '[PAD]', '[CLS]', '[SEP]', '[MASK]']\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=35000, special_tokens=special_tokens)\n",
    "\n",
    "# train the tokenizer\n",
    "tokenizer.model = models.WordPiece(unk_token=\"[UNK]\")\n",
    "tokenizer.train(['/Data/deeksha/disha/code_p/transformers/train.en'], trainer=trainer)\n",
    "tokenizer.train(['/Data/deeksha/disha/code_p/transformers/train.fr'], trainer=trainer)\n",
    "\n",
    "# post-processing\n",
    "cls_token_id = tokenizer.token_to_id('[CLS]')\n",
    "sep_token_id = tokenizer.token_to_id('[SEP]')\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
    ")\n",
    "encoding = tokenizer.encode(\"Hello how are you?\")\n",
    "\n",
    "## decoder\n",
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")\n",
    "tokenizer.decode(encoding.ids)\n",
    "\n",
    "## save the tokenizer\n",
    "tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "## load the tokenizer\n",
    "new_tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pssp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
